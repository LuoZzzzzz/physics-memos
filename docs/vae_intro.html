<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Variational Autoencoders</title>

  <!-- Link external CSS -->
  <link rel="stylesheet" href="styles.css" />

  <!-- MathJax LaTeX Support -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        tags: 'all'  // Enable automatic equation numbering
      }
    };
  </script>
  <script
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  ></script>
</head>
<body>
  <header>
    Variational Autoencoders
  </header>

  <main>
    <nav class="page-nav">
      <a href="index.html" class="btn">Index</a>
    </nav>

    <h1>A Simple Introduction</h1>

    <p>
      We have a dataset $\mathbf{X} = \{X_1, \dots, X_N\}$ that is drawn independently from the same probability distribution $\tilde{P}(X)$. 
      We wish to model the distribution $\tilde{P}(X)$ using the following ingredients:
    </p>

    <ol>
      <li>Latent distribution, $P(z) = N(0, I)$, where $N(0, I)$ is a normal distribution with mean $0$ and unit covariance matrix.</li>
      <li>A conditional distribution $P(X|z)$. This conditional distribution is generally represented by a neural network, and will contain tunable parameters $\theta$, so $P(X|z, \theta)$ will be a more appropriate expression.</li>
    </ol>

    <p>
      The marginal distribution of our approximation in $X$ space (data space) is then:
    </p>
    $$
      P(X) = \int P(X|z) P(z) \, dz
    $$

    <p>
      A naive approach to optimisation will be as follows:
    </p>
    <ol>
      <li>Sample $M$ points, $z_1, \dots, z_M$ from $P(z)$.</li>
      <li>Calculate the conditional probability for each point, $P(X|z_j)$.</li>
      <li>Take the mean probability distribution across the $M$ points, $\sum_{j=1}^{M} P(X|z_j)$. This gives us an <b>estimate</b> $P_{est}(X)$ for the marginal distribution $P(X)$ of our model. Note that the effect of the $P(z)$ term is reflected in the frequency that each $z_j$ point is drawn, so a simple average across all points will do.</li>
      <li>Calculate $P_{est}(X_i)$ for each of the data points $X_i$.</li>
      <li>Find the total (estimated) log likelihood that the model generates the dataset $\mathbf{X}$, $\log P(\mathbf{X}) = \sum_{i=1}^N \log P_{est}(X_i)$.</li>
    </ol>

    <p>
      We can then perform backpropagation and gradient descent on the objective $\log P(\mathbf{X})$ to tune the parameters of the conditional distribution, $\theta$. 
    </p>

    <h2>Efficient Sampling</h2>
    <p>  
      The main issue with the above approach is that $P(X_i|z_j)$ will be close to $0$ for most of the $z_j$ we sample. 
      This means we have to sample a ridiculously large number of points in order to obtain a reasonably accurate estimate of $P(X)$. 
    </p>

    <p>
      Suppose we somehow had access to the posterior distribution of our model, $P(z|X)$. 
      What if we sampled $z$ according to the conditional distributions $P(z|X_i)$ for $i = 1, \dots, N$, instead of the latent distribution $P(z)$? 
    </p>

    <p>
      The key idea is this: if $P(z_j|X_i)$ is large, then $P(X_i|z_j)$ is also likely to be large, because regions of large $P(z)$ tend to link to regions of large $P(X)$. 
      Therefore, the terms in our sum will tend to be significant. 
    </p>

    <p>
      Another question arises: if we sample $z$ using this approach, will it still be distributed according to $P(z)$? 
      The distribution in this case will be $\frac{1}{N} \sum_{i=1}^N P(z|X_i)$. 
      Now, if $X$ was discrete, then $P(X_i)$ will manifest itself in the number of time $X_i$ occurs in the dataset $\mathbf{X}$, so the distribution becomes $\frac{1}{D} \sum_{k=1}^D P(z|X_k) P(X_k) = P(z)$, where $D$ is the number of different states of $X$. 
      Therefore, at least in the discrete case, $z$ is still drawn from $P(z)$. 
      The idea is the same with continuous $X$, except that $P(X)$ now manifests in the distribution of the points $X_i$ in $X$ space. 
    </p>

    <h2>The Intractable Posterior</h2>
    <p>
      We now run into another problem - the distribution $P(z|X)$, known as the posterior, is intractable. 
      What do we do?
    </p>

    <p>
      The solution is to borrow a page out of variational Bayes. 
      We introduce another conditional distribution $Q(z|X)$. 
      At first, $Q(z|X)$ will look nothing like $P(z|X)$ (which is still unknown to us). 
      Let us now introduce the KL divergence between $Q(z|X)$ and $P(z|X)$:
    </p>
    $$
      D[Q(z|X) || P(z|X)] = \int Q(z|X) \log \frac{Q(z|X)}{P(z|X)} \, dz
    $$

    <p>
      If we could somehow obtain an analytic expression for an estimate of the KL divergence, then we can "squeeze" the distributions $Q(z|X)$ and $P(z|X)$ so that they resemble each other more by minimising this quantity over all $X_i$ in $\mathbf{X}$. 
      The issue here of course is that the KL divergence also involves the intractable posterior, so it seems like we have gotten nowhere. 
    </p>

    <p>
      However, what we have written is actually super useful. 
      To see why, let us rewrite $P(z|X)$ using Bayes' rule:
    </p>
    $$
      P(z|X) = \frac{P(X|z)P(z)}{P(X)}
    $$ 

    <p>
      Taking logs on both sides give:
    </p>
    $$
      \log P(z|X) = \log P(X|z) + \log P(z) - \log P(X)
    $$

    <p>
      Plugging into the above equation gives:
    </p
    >
    $$
    \begin{aligned}
      &D[Q(z|X) || P(z|X)] \\[1em] 
      &= \int Q(z|X) [\log Q(z|X) - \log P(X|z) - \log P(z) + \log P(X)] \, dz
    \end{aligned}
    $$

    <p>
      Rearranging the above equation, we get:
    </p>
    $$
    \begin{aligned}
      &\log P(X) - D[Q(z|X) || P(z|X)] \\[1em]
      &= \int Q(z|X) \log P(X|z) \, dz - \int Q(z|X) [\log Q(z|X) - \log P(z)] \, dz \\
      &= \int Q(z|X) \log P(X|z) \, dz - D[Q(z|X) || P(z)] 
    \end{aligned}	
    $$

    <h2>Interpreting the Objective</h2>
    <p>
      Let's take a look at what we have here. 
      The left hand side of the equation contains the objective that we wish to maximise, $\log P(X)$. 
      However, there is also an <i>error term</i> represented by the KL divergence between the distributions $Q(z|X)$ and $P(z|X)$. Both of $\log P(X)$ and  are intractable on their own.
    </p>

    <p>
      The right hand side contains two quantities that we do know how to approximate - namely, the KL divergence between $Q(z|X)$ and the latent distribution $P(z)$, which is simply $N(0, I)$, and the expectation of $P(X|z)$ with respect to $Q(z|X)$. 
      $Q(z|X)$ is nothing but the output of our encoder for some given data input $X$, while $P(z|X)$ is simply the output of our decoder for some given latent vector $z$, so both of them can be computed easily. 
      It should also be noted that the expectation $\int Q(z|X) \log P(X|z) \, dz$ really defines a function of $X$, and our goal is to minimise its value over the $X_i$ in $\mathbf{X}$. 
      Remember: the end objective is to maximise $\sum_{i=1}^N \log P(X_i)$, so when we actually use the above objective during training we have to replace the random variable $X$ with a fixed training vector $X_i$, in which case both the left and right hand sides will become actual numbers.
    </p>

    <p>
      Now, if we use a sufficiently flexible mapping for $Q$, then as we train the network the $D[Q(z|X) || P(z|X)]$ error term will eventually become zero, at which point $Q(z|X)$ will come to exactly resemble $P(z|X)$ and the RHS becomes a perfect estimator for $\log P(X)$. 
      In light of this we may interpret this error term as the bias due to "incorrect sampling" - because we are drawing $z$ from $Q(z|X)$ when we really should be drawing it $P(z|X)$, our estimate will be skewed from the true value. 
    </p>

    <p>
      Now will be a good point to highlight what we have done so far. 
      We could very well have written down the equation for the objective directly and drawn the same conclusion. 
      What we have done however, was to present a chain of thoughts, starting with a naive approach and introducing new elements until we were eventually led to the expression for the objective. 
      While mathematically there is no difference, the second path feels much more natural, and introduces the meaning of each element in a much more contextual way. 
    </p>

    <h2>Another Interpretation of the Objective</h2>
    <p>
      There is another way we can interpret the objective function that parallels traditional autoencoders. 
    </p>

    <p>    
      The quantity $\int Q(z|X) \log P(X|z) \, dz$ is essentially the reconstruction loss of the autoencoder. 
      To maximise this quantity in a practical setting, we take an image $X_i$, feed it through the encoder neural network, and obtain a distribution $Q(z|X_i)$ (which in most practical implementations is a Gaussian parameterised by mean $\mu$ and variance $\sigma^2$ - $\mu$ and $\sigma^2$ are learnt during training). 
      We then sample a random $z_i$ from $Q(z|X_i)$ and feed it through the decoder neural network to obtain the distribution to obtain the "reconstructed" image, $\tilde{X}_i$.  
      Finally we calculate the mean squared error between $X_i$ and $\tilde{X}_i$ and perform backpropagation. 
      This is precisely the same training procedure as a vanilla autoencoder, except for the random sampling step in the latent space (why is the randomness necessary?).
    </p>

    <p>
      The $D[Q(z|X) || P(z)]$ term represents an effort to "squeeze" the conditional latent distribution for each individual image $X_i$ towards the latent distribution $P(z)$ (in this case $N(0, I)$). 
      When evaluated over the entire dataset, the effect is to squeeze the conditional latent distribution of the dataset $P(z|\mathbf{X}) = \frac{1}{N} \sum_{i=1}^N P(z|X_i)$ towards the $N(0, I)$. 
      This is essential for the inference phase, where we sample latent vectors $z$ directly from $N(0, I)$ and feed them into the decoder (thereby we are implicitly assuming that the latent distribution of the dataset is $N(0, I)$). 
    </p>

</body>
</html>