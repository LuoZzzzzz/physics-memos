<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Index Notation</title>

  <!-- Link external CSS -->
  <link rel="stylesheet" href="styles.css" />

  <!-- MathJax LaTeX Support -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        tags: 'all'  // Enable automatic equation numbering
      }
    };
  </script>
  <script
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  ></script>
</head>
<body>
  <header>
    Physics Memos
  </header>

  <main>
    <nav class="page-nav">
      <a href="index.html" class="btn">Index</a>
    </nav>

    <h1>Index Notation</h1>
    <p>
      These are a sequence of personal notes on index notation. The older notes were written when I just learnt the subject so they contain a lot of misconceptions, but I keep them here for record.
    </p>

    <h2>Date: 12/11/25</h2>
    <p>
      Always start with a metric $g$. Let us assume that we always pick a symmetric $g$. From the metric $g$ we are able to introduce the motion of a dot product, given by $A \cdot B = A^T g B$. 
    </p>

    <p>
      We then introduce the set of transformations $\Lambda$ that leaves the dot product of two vectors unchanged. The defining property is: $\Lambda^T g \Lambda = g$. 
    </p>

    <p>
      Now we introduce a notation that makes it easier to write the dot product. In matrix form, we note that the dot product may be written as $(g^T A)^T B$. But since $g$ is symmetric we might as well write $(gA)^T B$. Define the covariant 4-vector $A_\mu = [(gA)^T]_\mu$, and the contravariant 4-vector $A^\mu = [A]_\mu$. Square brackets highlight we are using ordinary matrix notation (in practice one should not mix matrix and index notation, but in this case we are providing a definition so we do it with care). In this notation the dot product is just $A_\mu A^\mu$. 
    </p>
    
    <p>
    <div style="border: 2px solid black; padding: 10px; background-color: #f0f0f0; width: fit-content;">
      <b>Note 1: </b> In general, because the metric $g$ can be whatever we want, the components of $A_\mu$ can be very different from the components of $A^\mu$. 
    </div>
    </p>

    <p>
    <div style="border: 2px solid black; padding: 10px; background-color: #f0f0f0; width: fit-content;">
      <b>Insight 1: </b> Whenever a Lorentz transformation appears transposed, it is because we are using the metric to move things around. All transposes should come paired with a metric. 
    </div>
    </p>

    <p>
    <div style="border: 2px solid black; padding: 10px; background-color: #f0f0f0; width: fit-content;">
      <b>Insight 2: </b> A metric remains a metric when sandwiched between a pair of transformations.
      <br><br>
      Therefore, the notation $g'_{\mu\nu} = \Lambda^\lambda_{\_\mu} g_{\lambda \kappa} \Lambda^\kappa_{\_\nu}$ is logically sound. $g'_{\mu\nu}$ is simply the metric used in a different frame, so it is still a tensor of rank $(0,2)$, hence both indices are lower - in agreement with the cancellation rule for indices. Note also that in this case we sum over columns of both $\Lambda^\lambda_{\_\mu}$ and $\Lambda^\kappa_{\_\nu}$. This just means that we are doing matrix multiplication with the transpose of $\Lambda^\lambda_{\_\mu}$. This brings us back to insight 1, which tells us that whenever we see a (transformation, metric) pair, the transformation on the left is implicitly assumed to be transposed. Because the transformation to the left of the metric is transposed, the summation index $\lambda$ goes in the upper position (so we sum over columns of the left $\Lambda$ as we would for regular matrix multiplication $\Lambda^T g$), and it cancels nicely with the $\lambda$ in $g$ to give a $\mu$ in the lower position. 
    </div>
    </p>

    <p>
    <div style="border: 2px solid black; padding: 10px; background-color: #f0f0f0; width: fit-content;">
      <b>Insight 3: </b> Summation indices always come in upper-lower pairs. This implicitly enforces the shape of adjacent blocks to match. Therefore, to every valid equation written in index notation there is a corresponding valid equation in matrix notation. More importantly, each index position also carries transformation meaning, and ensures that we contract tensors in a physically sensible way.</b> 
      <br><br>
      As an illustration, consider the product $\Lambda^T \Lambda$. This is perfectly valid as far as matrix multiplication is concerned, but physically it has no meaning. However, if we write this in matrix notation, we get: $\Lambda^\kappa_{\_\mu} \Lambda^\kappa_{\_\nu}$, and we see immediately that there is a problem because the summation indices $\kappa$ are in the wrong position and therefore cannot contract (if for whatever reason we do see this we have to interpret $\kappa$ as a free variable, not a summation variable). On the other hand, something like $\Lambda^T g \Lambda$ does make physical sense and we see that its corresponding form in index notation $\Lambda^\lambda_{\_\mu} g_{\lambda \kappa} \Lambda^\kappa_{\_\nu}$ does lead to a nice cancellation of indices.
    </div>
    </p>

    <p>
    <div style="border: 2px solid black; padding: 10px; background-color: #f0f0f0; width: fit-content;">
      <b>Insight 4: </b> To transpose a boost matrix, just flip the upper and lower indices. However, note that this only ever makes sense if the tensor to the right is a metric. 
      <br><br>
      The matrix multiplication $S^T T$ is well defined for any two 4 by 4 matrices $S$ and $T$, so in index notation $ST$ will be $S^\mu_{\_\kappa} T^\kappa_{\_\nu}$ while $S^T T$ will be $S^\kappa_{\_\mu} T^\kappa_{\_\nu}$. However, if $S$ and $T$ were transformations, then $S^T T$ does not have any physical meaning and this is apparent from the index mismatch. As we have noted, transposes in relativity always come with a metric next to it. That's why metrics have two lower indices - so that these lower indices can cancel properly with both upper indices to its left and right. 
    </div>
    </p>

    <p>
    <div style="border: 2px solid black; padding: 10px; background-color: #f0f0f0; width: fit-content;">
      <b>Insight 5: </b> Commutativity is enforced by order of indices.
      <br><br>
      In general $ST$ and $TS$ are not the same. This is also clear in index notation: $ST$ will be $S^\mu_{\_\kappa} T^\kappa_{\_\nu}$ while $TS$ will be $S^\kappa_{\_\nu} T^\mu_{\_\kappa}$. Assuming that we are to the right of the metric, the tensor with the free index in the upper position is the one that is on the left in the corresponding matrix equation.
    </div>
    </p>

    <p>
      Now, on to transformation matrices $\Lambda$. Let's do everything in matrix form then we will introduce the index notation. Since we are working in matrix form all vectors are 4-vectors (the notion of covariant is really a convenient way of packaging the metric and $A^T$ into a single entity), $A' = \Lambda A$. Consequently for the covariant 4-vector $(gA')^T$:
      $$
      \begin{align} 
      (g A')^T & = (g \Lambda A)^T \\
      &= (g \Lambda g^{-1} g A)^T \\
      &= (g A)^T  (g \Lambda g^{-1})^T \\
      &= (gA)^T (g^{-1})^T \Lambda^T g^T \\
      &= (gA)^T g^{-1} \Lambda^T g \\
      &= (gA)^T \Lambda^{-1}
      \end{align} 
      $$
      where we have noted that the inverse of a symmetric matrix is also symmetric. The last line follows directly from the group definition $\Lambda^T g \Lambda = g$. 
    </p>

    <p>
      Therefore under Lorentz transformation, contravariant 4-vectors transform via right multiplication (ie. the matrix acts to its right) by $\Lambda$ while covariant 4-vectors transform by left multiplication by $\Lambda^{-1}$.
    </p>

    <p>
      In index notation, a contravariant 4-vector is transformed like: 
      $$A'^\mu = \Lambda^\mu_{\_\nu} A^\nu$$
      A covariant 4-vector is transformed like:
      $$A'_\mu = (\Lambda^{-1})^\nu_{\_\mu} A_\nu = \Lambda^{\_\nu}_{\mu} A_\nu$$
      We have defined:
      $$\Lambda^{\_\nu}_{\mu} = (\Lambda^{-1})^\nu_{\_\mu} = g^{\nu\lambda} \Lambda^\kappa_{\_\lambda} g_{\kappa\mu}$$
      As with $A^\mu$ and $A_\mu$, we can lower and raise $\Lambda^\mu_{\_\nu}$ and $\Lambda^{\_\mu}_\nu$ into one another using the metric $g$. This falls right out of the definition of $\Lambda^{\_\nu}_{\mu}$ above. As we can also check, the index works out nicely. Note also, that in the above notation we are really saying: 
      $$\Lambda^{\_\nu}_{\mu} = g^{\nu\lambda} (\Lambda^T)^\lambda_{\_\kappa} g_{\kappa\mu} = g^{\nu\lambda} \Lambda^\kappa_{\_\lambda} g_{\kappa\mu}$$
    </p>

    <p>
    <div style="border: 2px solid black; padding: 10px; background-color: #f0f0f0; width: fit-content;">
      <b>Note 2: </b> Note the order of the indices. The first index of $g^{\text{xx}}$ is $\nu$ because when we actually multiply $A_\nu$ by $g^{-1}$ the summation is down columns. Therefore, the summation index $\nu$ must run down columns, so its in the first position. There's some things to clarify here: if we do left to right, the order is covariant $\rightarrow$ contravariant $\rightarrow$ boost $\rightarrow$ covariant?
    </div>
    </p>

    <p>
      However, the first notation with $\Lambda^T$ will break our rules for cancelling indices, so instead we represent the transpose $\Lambda^T$ by swapping the positions of the $\lambda$ and $\kappa$ indices. 
    </p>

    <p>
    <div style="border: 2px solid black; padding: 10px; background-color: #f0f0f0; width: fit-content;">
      <b>Note 3: </b> Note that the left-right ordering of the indices in the transformation tensor matters a lot! If the order is (upper, left), (lower, right), then our tensor acts on contravariant 4-vectors, and if the order is (upper, right), (lower, left), then our tensor acts on covariant 4-vectors. Not only so, the matrix elements we use for the former belong to of $\Lambda$, while the matrix elements we use for the latter belong to $\Lambda ^{-1}$.
    </div>
    </p>

    <p>
      Overall the idea is as follows. Physically, there's really only one 4-vector at first - the contravariant one. However, we are interested in norms, so we try to make the contravariant 4-vector a friend, so that when we put them together they make the norm-squared. This friend is called the covariant 4-vector. It turns out that for the product of the contravariant and covariant 4-vectors to remain invariant, the covariant 4-vector must transform in a special way. What we have done above is to come up with a notation that automatically enforces the correct transformation properties through index positioning: upper indices transform with $\Lambda$ ($\Lambda^\mu_{\_\nu}$), lower indices transform with $\Lambda^{-1}$ ($\Lambda^{\_\mu}_\nu$), and the Einstein summation convention and cancellation rule ensures that only physically meaningful contractions (those that yield Lorentz invariants) are possible.
    </p>

    <h2>Date: 06/11/25</h2>
    <p>
      $A^\mu$: column 4-vector. Capital letters stand for vectors.
    </p>
    <p>
      $a^\mu$: components of $A^\mu$. Lowercase letters stand for components.
    </p>
    
    <p>
      The scalar product of two 4-vectors is defined using a metric $g$:
      $$A^\mu \cdot B^\mu = g_{\mu \nu} a^\mu b^\nu$$
      $g$ is a 4 by 4 matrix. We can check that this is really just a bilinear form:
      $$\vec{A}^T g \vec{B} = a_i (gB)_i = a_i g_{ij} b_j$$
      $i$ is to $\mu$ is to row as $j$ is to $\nu$ is to column. In the world of matrices, two $j$s is a right multiplication to get a column vector and two $i$s is a left multiplication to get a row vector, assuming the conventional $ij$.
    </p>

    <p>
      If we move the $\mu$ down we get the row 4-vector $A_\mu$. Its components are defined by:
      $$a_\mu = g_{\mu \nu} a^\mu$$
      In vector form this is:
      $$A_\mu = (gA^\mu)^T$$
      The first definition does not tell you anything about the shape of $A_\mu$. It seems you need both lines.
    </p>

    <p>
      $a_\mu$ is to $A_\mu$ what $a^\mu$ is to $A^\mu$. Note that the position of the index  in the component also changes depending on the type of 4-vector. **In the world of components**, think of subscript as _left to right_ and superscript as _top to bottom_. In fact, if you think about it, $\mu$ in $a^\mu$ and $A^\mu$ do different things! In $a^\mu$ $\mu$ serves as an index - zero, one, two, three. In $A^\mu$ $\mu$ serves as an indicator that "I am a 4-vector".
    </p>

    <p>
      Raising the indices of $g_{\mu \nu}$ gives $g^{\mu \nu}$. This is, by the definition, the $\mu$ row $\nu$ column element of $g^{-1}$. Therefore:
      $$g^{\mu \alpha} g_{\alpha \nu} = \delta ^\mu _\nu$$
      The LHS is just matrix multiplication in summation notation. The RHS is a 4 by 4 identity matrix. When deciphering the RHS recall that superscript means top down and subscript means left right. So $\delta ^\mu _\nu = I_{\mu \nu}$. We get to do this with $I$ because it is its own inverse. We cannot do the same with $g$. If we did then we run into trouble trying to get a symbol for components of $g^{-1}$. 
    </p>

    <p>
      If we have a row 4-vector we can turn it back into a column 4-vector by raising its indices:
      $$a^\mu = g^{\mu \nu} a_\nu$$
      This looks weird because $a_\nu$ is a component of a row 4-vector; it seems we are putting a matrix before a row vector and the shapes don't match. Things are actually fine because there are no matrices here - everybody is a number. The matrix equation is:
      $$A^\mu = g^{-1} A_\mu^T$$
      Again, the first equation does not tell us anything about the shape of the 4-vectors. 
    </p>


</body>
</html>